---
layout: post
title: k8s 覆盖网络相关问题
categories:
  - k8s
  - fannel
---


> [参考文档](http://dockone.io/article/618)
#### k8s 本地环境搭建
```bash
git clone https://github.com/rootsongjc/kubernetes-vagrant-centos-cluster.git
cd kubernetes-vagrant-centos-cluster
vagrant up
```

#### k8s 不同节点之间如何通信的?
> 目前采用 vagrant + centos7 进行测试的
环境: 
-  master: node1 : 172.17.8.101
-  node: node2 : 172.17.8.102
- traefik 负责 ingress  (在 node2 节点上)

场景: 新建了两个 nginx 构成负载均衡,但是发现其中一个nginx无法访问,k8s当中同一个pod 之间容器不应该是互通的吗,怎嘛会有一个不通那?

问题排查: 
通过 traefik 看到 nginx 的两个后端分别为(域名: http://nginx.demo.com/):
    
    http://172.33.14.8:80	1
    http://172.33.64.2:80	1

在两个主机当中分别 curl 测试,发现其中一个老是无法正常访问.那马问题肯定出现在服务器集群无法互通问题上面!!!


这里我们首先排查 etcd里面是否有主机信息: 

    etcdctl ls /kube-centos/network/subnets
    # 发现为空,说明 etcd 里面有问题

    # 我们在 Vagrantfile 文件当中看到如下信息,分别执行他们
    cat > /etc/etcd/etcd-init.sh<<EOF
    #!/bin/bash
    etcdctl mkdir /kube-centos/network
    etcdctl mk /kube-centos/network/config '{"Network":"172.33.0.0/16","SubnetLen":24,"Backend":{"Type":"host-gw"}}'
    EOF

    chmod +x /etc/etcd/etcd-init.sh
    echo 'start etcd...'
    systemctl daemon-reload
    systemctl enable etcd
    systemctl start etcd

    echo 'create kubernetes ip range for flannel on 172.33.0.0/16'
    /etc/etcd/etcd-init.sh
    etcdctl cluster-health
    etcdctl ls /

    # 最后我们重启各个服务器
    # 发现 etcd正常
    etcdctl ls /kube-centos/network/subnets
    /kube-centos/network/subnets/172.33.14.0-24
    /kube-centos/network/subnets/172.33.64.0-24

原理: k8s 使用 flanneld 使各个主机之间构成覆盖网络.

    # 在 Vagratfile 可以看到如下的 配置文件
    echo 'create flannel config file...'

    cat > /etc/sysconfig/flanneld <<EOF
    # Flanneld configuration options
    FLANNEL_ETCD_ENDPOINTS="http://172.17.8.101:2379"
    FLANNEL_ETCD_PREFIX="/kube-centos/network"
    FLANNEL_OPTIONS="-iface=eth1"
    EOF

    echo 'enable flannel with host-gw backend'
    rm -rf /run/flannel/
    systemctl daemon-reload
    systemctl enable flanneld
    systemctl start flanneld

flannel 会在各个主机上面运行,确保将信息注册到 etcd 当中,如果 etcd 挂了,整个 k8s 服务也就无法正常工作了,所以很多时候都是使用 etcd 集群

同时我们看一下路由表:

    # node1
    [root@node1 ~]# route
    Kernel IP routing table
    Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
    default         gateway         0.0.0.0         UG    100    0        0 eth0
    default         gateway         0.0.0.0         UG    101    0        0 eth2
    10.0.2.0        0.0.0.0         255.255.255.0   U     100    0        0 eth0
    172.17.8.0      0.0.0.0         255.255.255.0   U     100    0        0 eth1
    172.33.14.0     0.0.0.0         255.255.255.0   U     0      0        0 docker0
    172.33.64.0     node2           255.255.255.0   UG    0      0        0 eth1
    192.168.0.0     0.0.0.0         255.255.255.0   U     100    0        0 eth2

    # node2
    [root@node2 ~]# route
    Kernel IP routing table
    Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
    default         gateway         0.0.0.0         UG    100    0        0 eth0
    default         gateway         0.0.0.0         UG    101    0        0 eth2
    10.0.2.0        0.0.0.0         255.255.255.0   U     100    0        0 eth0
    172.17.8.0      0.0.0.0         255.255.255.0   U     100    0        0 eth1
    172.33.14.0     node1           255.255.255.0   UG    0      0        0 eth1
    172.33.64.0     0.0.0.0         255.255.255.0   U     0      0        0 docker0
    192.168.0.0     0.0.0.0         255.255.255.0   U     100    0        0 eth2

可以看到路由表里面添加了一个指向 node2,node1 的一条信息
